# Real-Time Stock Market Data Pipeline and Alerting System (AWS)

This project simulates a real-time stock market data pipeline that ingests, processes, and analyzes stock market data to generate alerts based on user-defined criteria. It's designed to be a simplified, yet relatively robust, example of a system commonly used in financial institutions for algorithmic trading, risk management, and market surveillance.

This version of the project has been enhanced to leverage various AWS services, making it more scalable, resilient, and representative of a production-like environment.

## Project Architecture

The system consists of the following components:

1.  **Data Simulator:** A Python script (`data_simulator/simulator.py`) that generates realistic stock market data (prices, volumes) for a set of predefined stock symbols and publishes it to an Amazon MSK topic.
2.  **Amazon MSK (Managed Streaming for Kafka):** A fully managed Kafka service that acts as the central message broker, receiving the simulated stock data and making it available to downstream consumers.
3.  **Spark Streaming Application:** A Spark Streaming application (`spark_processing/spark_app.py`) running on EC2 instances that consumes data from the MSK topic, performs windowed aggregations (e.g., average price, volume over 5 minutes), calculates a 5-period Simple Moving Average (SMA), and stores the results in Amazon S3 (data lake) and Amazon RDS for PostgreSQL.
4.  **Alerting Engine:** A Python application (`alerting_engine/alerter.py`) that reads the aggregated data from PostgreSQL, checks for predefined alert conditions (e.g., price crossing above SMA), publishes alerts to an Amazon MSK topic, and stores alerts in Amazon DynamoDB.
5.  **AWS Lambda (Alert Consumer):** An AWS Lambda function that is triggered by new messages in the MSK alerts topic. It consumes alerts and sends notifications (this can be extended to send notifications via email using SES, SMS using SNS, or other channels).
6.  **Amazon DynamoDB:** A NoSQL database used to store alerts generated by the Alerting Engine.
7.  **Amazon S3 (Data Lake):** Stores processed data in a structured format (Parquet) for long-term storage, analysis, and potential use by other applications.
8.  **Web Application (Optional):** A serverless web application using AWS API Gateway and AWS Lambda that fetches and displays alerts from DynamoDB, providing a user interface to view alerts.

## Technologies Used

*   **Programming Languages:** Python
*   **Amazon MSK:** Fully managed Apache Kafka service.
*   **Apache Spark:** Unified analytics engine for large-scale data processing (running on EC2 instances).
*   **Amazon RDS for PostgreSQL:** Managed relational database service.
*   **Amazon DynamoDB:** NoSQL database service.
*   **Amazon S3:** Object storage for data lake.
*   **AWS Lambda:** Serverless compute service for running the alert consumer and optional web app backend.
*   **Amazon API Gateway (Optional):** Fully managed service to create, publish, maintain, monitor, and secure APIs.
*   **Docker:** Containerization platform for easy development and deployment.
*   **AWS CloudFormation (Optional):** Infrastructure as Code service to define and manage AWS resources.

## Prerequisites

*   An AWS account.
*   AWS CLI installed and configured.
*   Docker (for local development).
*   Docker Compose (for local development).
*   Familiarity with AWS services (MSK, EC2, RDS, S3, Lambda, DynamoDB, API Gateway, CloudFormation, IAM).

## Setup and Installation (Local Development with Docker Compose)

1.  **Clone the repository:**

    ```bash
    git clone <repository_url>
    cd real-time-stock-alerting
    ```

2.  **Build Docker images:**

    ```bash
    docker-compose build
    ```

3.  **Start the application:**

    ```bash
    docker-compose up -d
    ```

    This command will start all the services defined in `docker-compose.yml` (data simulator, Spark Master, alerter, and the optional web app if you choose to use it locally). The Spark application needs to be manually submitted to the Spark Master, and AWS services are mocked for local development.

## Setup and Installation (AWS Deployment)

1.  **Create AWS Resources:**
    *   **Amazon MSK Cluster:** Create an MSK cluster with the necessary number of brokers and appropriate configuration.
    *   **Amazon EC2 Instances:**
        *   Launch EC2 instances for the Spark Master and Workers (optional). You might use an Auto Scaling Group for the workers.
        *   Install Spark on the instances and configure them to connect to the MSK cluster.
    *   **Amazon RDS for PostgreSQL:** Create an RDS instance with a PostgreSQL database to store aggregated data.
    *   **Amazon DynamoDB Table:** Create a DynamoDB table to store alerts.
    *   **Amazon S3 Bucket:** Create an S3 bucket for the data lake.
    *   **AWS Lambda Functions:**
        *   Create a Lambda function for the Alert Consumer.
        *   (Optional) Create a Lambda function for the web app backend.
    *   **Amazon API Gateway (Optional):** Create an API Gateway endpoint to trigger the web app Lambda function.
    *   **IAM Roles and Policies:** Create IAM roles with appropriate permissions for each component (Spark, Alerting Engine, Lambda functions).

2.  **Configure Environment Variables:**
    *   Update the environment variables in your application code (or in the `docker-compose.yml` for local development) to point to the correct AWS resource endpoints (MSK brokers, RDS instance, DynamoDB table, S3 bucket).

3.  **Deploy Applications:**
    *   **Spark Application:** Package the Spark application (`spark_app.py`) and its dependencies. You can either copy it to the Spark Master node and use `spark-submit` there or submit it remotely using the Spark REST API.
    *   **Alerting Engine:** Package the alerting engine (`alerter.py`) and its dependencies. You can deploy it to an EC2 instance, or package it as a Docker container and deploy it to a service like ECS or EKS.
    *   **Lambda Functions:** Deploy the Lambda function code (Alert Consumer and optional web app backend) to AWS Lambda.

4.  **Configure Triggers:**
    *   Configure the Alert Consumer Lambda function to be triggered by new messages in the MSK `stock-alerts` topic.
    *   (Optional) Configure the API Gateway endpoint to trigger the web app Lambda function.

## Deployment with AWS CloudFormation (Optional)

You can define your entire infrastructure in an AWS CloudFormation template. This allows you to automate the creation and management of all the necessary AWS resources.

## Configuration

The application uses environment variables for configuration. Here are the key variables:

*   **`KAFKA_BROKERS`:** Comma-separated list of MSK broker endpoints.
*   **`POSTGRES_HOST`:** (For local development with PostgreSQL) Hostname of the PostgreSQL database.
*   **`POSTGRES_PORT`:** (For local development with PostgreSQL) Port number of the PostgreSQL database.
*   **`POSTGRES_DB`:** (For local development with PostgreSQL) Name of the PostgreSQL database.
*   **`POSTGRES_USER`:** (For local development with PostgreSQL) Username for the PostgreSQL database.
*   **`POSTGRES_PASSWORD`:** (For local development with PostgreSQL) Password for the PostgreSQL database.
*   **`DYNAMODB_TABLE`:** Name of the DynamoDB table for storing alerts.
*   **`S3_BUCKET`:** Name of the S3 bucket for the data lake.

## Error Handling and Logging

*   The Spark application logs errors to files in the `/tmp` directory within the container (for local development) and to Amazon S3 (in the AWS deployment).
*   The data simulator, alerting engine, and Lambda functions log messages to standard output, which can be captured in CloudWatch Logs.
*   You can view the logs locally using `docker-compose logs <service_name>` or in the AWS Management Console for CloudWatch Logs.

## Scalability and Performance

*   **Amazon MSK:** MSK is a fully managed service that can be scaled by adding more brokers.
*   **Spark on EC2:** You can scale the Spark cluster by adding more worker nodes (potentially using an Auto Scaling Group).
*   **AWS Lambda:** Lambda functions scale automatically based on the number of incoming requests.
*   **Amazon DynamoDB:** DynamoDB is a highly scalable NoSQL database.
*   **Amazon S3:** S3 is designed for massive scalability and durability.
*   **Amazon RDS:** You can scale RDS instances vertically (by increasing instance size) or horizontally (by adding read replicas).

## Security Considerations

*   **IAM Roles and Policies:** Use IAM roles and policies to control access to AWS resources. Grant least privilege to each component.
*   **Network Security:** Use security groups to control inbound and outbound traffic to your EC2 instances and RDS instance. Consider using a VPC to isolate your resources.
*   **Encryption:** Enable encryption at rest for S3, RDS, and DynamoDB. Use TLS/SSL for communication with MSK and other services.
*   **Authentication and Authorization:** Implement authentication and authorization for your web application (if applicable).
*   **Vulnerability Scanning:** Regularly scan your application code and dependencies for vulnerabilities.
